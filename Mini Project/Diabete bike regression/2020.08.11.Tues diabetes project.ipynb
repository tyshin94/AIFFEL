{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1. 손수 설계하는 선형회귀, 당뇨병 수치 맞추기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 데이터 가져오기\n",
    "- sklearn.datasets의 load_diabetes에서 데이터를 가져와주세요.\n",
    "- diabetes의 data를 df_X에, target을 df_y에 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  \n",
       "0 -0.002592  0.019908 -0.017646  \n",
       "1 -0.039493 -0.068330 -0.092204  \n",
       "2 -0.002592  0.002864 -0.025930  \n",
       "3  0.034309  0.022692 -0.009362  \n",
       "4 -0.002592 -0.031991 -0.046641  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "df_X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "print(df_X.shape)\n",
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "df_y = diabetes.target\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 모델에 입력할 데이터 x 준비하기\n",
    "- df_X에 있는 값들을 numpy array로 변환해서 저장해주세요.\n",
    "- df_y에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(df_X)\n",
    "y = np.array(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]] [151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10) (442,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) train 데이터와 test 데이터로 분리하기\n",
    "- X와 y 데이터를 각각 train 데이터와 test 데이터로 분리해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) 모델 준비하기\n",
    "- 입력 데이터 개수에 맞는 가중치 W와 b를 준비해주세요.\n",
    "- 모델 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2021939  0.23610221 0.95504645 0.83294779 0.08297456 0.44071764\n",
      " 0.22304937 0.44272317 0.28013363 0.31771774] 0.8125269197579942\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "print(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 함구 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) 손실함수 loss 정의하기\n",
    "- 손실함수를 MSE 함수로 정의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) 기울기를 구하는 gradient 함수 구현하기\n",
    "- 기울기를 계산하는 gradient 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    # N은 data의 개수\n",
    "    N = len(X)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [-1.37158509 -0.31186779 -4.28781416 -3.22622608 -1.54649522 -1.26858675\n",
      "  2.88758613 -3.14585594 -4.13800564 -2.79497561]\n",
      "db: -302.6419144862759\n"
     ]
    }
   ],
   "source": [
    "dW, db = gradient(X, W, b, y)\n",
    "print(\"dW:\", dW)\n",
    "print(\"db:\", db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) 하이퍼 파라미터인 학습률 설정하기 \n",
    "- 학습률, learning rate 를 설정해주세요.\n",
    "- 만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요.    \n",
    "\n",
    "## (9) 모델 학습하기\n",
    "- 정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "- loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "- 입력하는 데이터인 X에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 3311.1759\n",
      "Iteration 20 : Loss 3306.1979\n",
      "Iteration 30 : Loss 3301.3009\n",
      "Iteration 40 : Loss 3296.4830\n",
      "Iteration 50 : Loss 3291.7424\n",
      "Iteration 60 : Loss 3287.0772\n",
      "Iteration 70 : Loss 3282.4857\n",
      "Iteration 80 : Loss 3277.9663\n",
      "Iteration 90 : Loss 3273.5173\n",
      "Iteration 100 : Loss 3269.1370\n",
      "Iteration 110 : Loss 3264.8240\n",
      "Iteration 120 : Loss 3260.5768\n",
      "Iteration 130 : Loss 3256.3939\n",
      "Iteration 140 : Loss 3252.2739\n",
      "Iteration 150 : Loss 3248.2154\n",
      "Iteration 160 : Loss 3244.2171\n",
      "Iteration 170 : Loss 3240.2777\n",
      "Iteration 180 : Loss 3236.3960\n",
      "Iteration 190 : Loss 3232.5707\n",
      "Iteration 200 : Loss 3228.8006\n",
      "Iteration 210 : Loss 3225.0846\n",
      "Iteration 220 : Loss 3221.4216\n",
      "Iteration 230 : Loss 3217.8105\n",
      "Iteration 240 : Loss 3214.2503\n",
      "Iteration 250 : Loss 3210.7398\n",
      "Iteration 260 : Loss 3207.2782\n",
      "Iteration 270 : Loss 3203.8644\n",
      "Iteration 280 : Loss 3200.4974\n",
      "Iteration 290 : Loss 3197.1765\n",
      "Iteration 300 : Loss 3193.9006\n",
      "Iteration 310 : Loss 3190.6689\n",
      "Iteration 320 : Loss 3187.4806\n",
      "Iteration 330 : Loss 3184.3349\n",
      "Iteration 340 : Loss 3181.2309\n",
      "Iteration 350 : Loss 3178.1678\n",
      "Iteration 360 : Loss 3175.1449\n",
      "Iteration 370 : Loss 3172.1615\n",
      "Iteration 380 : Loss 3169.2168\n",
      "Iteration 390 : Loss 3166.3101\n",
      "Iteration 400 : Loss 3163.4408\n",
      "Iteration 410 : Loss 3160.6082\n",
      "Iteration 420 : Loss 3157.8116\n",
      "Iteration 430 : Loss 3155.0503\n",
      "Iteration 440 : Loss 3152.3239\n",
      "Iteration 450 : Loss 3149.6316\n",
      "Iteration 460 : Loss 3146.9729\n",
      "Iteration 470 : Loss 3144.3472\n",
      "Iteration 480 : Loss 3141.7540\n",
      "Iteration 490 : Loss 3139.1926\n",
      "Iteration 500 : Loss 3136.6627\n",
      "Iteration 510 : Loss 3134.1635\n",
      "Iteration 520 : Loss 3131.6948\n",
      "Iteration 530 : Loss 3129.2558\n",
      "Iteration 540 : Loss 3126.8463\n",
      "Iteration 550 : Loss 3124.4656\n",
      "Iteration 560 : Loss 3122.1133\n",
      "Iteration 570 : Loss 3119.7890\n",
      "Iteration 580 : Loss 3117.4923\n",
      "Iteration 590 : Loss 3115.2226\n",
      "Iteration 600 : Loss 3112.9797\n",
      "Iteration 610 : Loss 3110.7630\n",
      "Iteration 620 : Loss 3108.5722\n",
      "Iteration 630 : Loss 3106.4068\n",
      "Iteration 640 : Loss 3104.2665\n",
      "Iteration 650 : Loss 3102.1510\n",
      "Iteration 660 : Loss 3100.0597\n",
      "Iteration 670 : Loss 3097.9925\n",
      "Iteration 680 : Loss 3095.9489\n",
      "Iteration 690 : Loss 3093.9286\n",
      "Iteration 700 : Loss 3091.9312\n",
      "Iteration 710 : Loss 3089.9564\n",
      "Iteration 720 : Loss 3088.0039\n",
      "Iteration 730 : Loss 3086.0734\n",
      "Iteration 740 : Loss 3084.1645\n",
      "Iteration 750 : Loss 3082.2770\n",
      "Iteration 760 : Loss 3080.4105\n",
      "Iteration 770 : Loss 3078.5648\n",
      "Iteration 780 : Loss 3076.7395\n",
      "Iteration 790 : Loss 3074.9345\n",
      "Iteration 800 : Loss 3073.1493\n",
      "Iteration 810 : Loss 3071.3838\n",
      "Iteration 820 : Loss 3069.6377\n",
      "Iteration 830 : Loss 3067.9106\n",
      "Iteration 840 : Loss 3066.2025\n",
      "Iteration 850 : Loss 3064.5129\n",
      "Iteration 860 : Loss 3062.8417\n",
      "Iteration 870 : Loss 3061.1886\n",
      "Iteration 880 : Loss 3059.5534\n",
      "Iteration 890 : Loss 3057.9358\n",
      "Iteration 900 : Loss 3056.3357\n",
      "Iteration 910 : Loss 3054.7527\n",
      "Iteration 920 : Loss 3053.1867\n",
      "Iteration 930 : Loss 3051.6375\n",
      "Iteration 940 : Loss 3050.1048\n",
      "Iteration 950 : Loss 3048.5884\n",
      "Iteration 960 : Loss 3047.0881\n",
      "Iteration 970 : Loss 3045.6038\n",
      "Iteration 980 : Loss 3044.1352\n",
      "Iteration 990 : Loss 3042.6821\n",
      "Iteration 1000 : Loss 3041.2443\n",
      "Iteration 1010 : Loss 3039.8217\n",
      "Iteration 1020 : Loss 3038.4141\n",
      "Iteration 1030 : Loss 3037.0212\n",
      "Iteration 1040 : Loss 3035.6429\n",
      "Iteration 1050 : Loss 3034.2790\n",
      "Iteration 1060 : Loss 3032.9294\n",
      "Iteration 1070 : Loss 3031.5938\n",
      "Iteration 1080 : Loss 3030.2722\n",
      "Iteration 1090 : Loss 3028.9643\n",
      "Iteration 1100 : Loss 3027.6699\n",
      "Iteration 1110 : Loss 3026.3890\n",
      "Iteration 1120 : Loss 3025.1213\n",
      "Iteration 1130 : Loss 3023.8667\n",
      "Iteration 1140 : Loss 3022.6250\n",
      "Iteration 1150 : Loss 3021.3961\n",
      "Iteration 1160 : Loss 3020.1799\n",
      "Iteration 1170 : Loss 3018.9762\n",
      "Iteration 1180 : Loss 3017.7848\n",
      "Iteration 1190 : Loss 3016.6056\n",
      "Iteration 1200 : Loss 3015.4385\n",
      "Iteration 1210 : Loss 3014.2833\n",
      "Iteration 1220 : Loss 3013.1399\n",
      "Iteration 1230 : Loss 3012.0081\n",
      "Iteration 1240 : Loss 3010.8879\n",
      "Iteration 1250 : Loss 3009.7791\n",
      "Iteration 1260 : Loss 3008.6815\n",
      "Iteration 1270 : Loss 3007.5950\n",
      "Iteration 1280 : Loss 3006.5196\n",
      "Iteration 1290 : Loss 3005.4551\n",
      "Iteration 1300 : Loss 3004.4013\n",
      "Iteration 1310 : Loss 3003.3581\n",
      "Iteration 1320 : Loss 3002.3255\n",
      "Iteration 1330 : Loss 3001.3033\n",
      "Iteration 1340 : Loss 3000.2913\n",
      "Iteration 1350 : Loss 2999.2896\n",
      "Iteration 1360 : Loss 2998.2979\n",
      "Iteration 1370 : Loss 2997.3162\n",
      "Iteration 1380 : Loss 2996.3443\n",
      "Iteration 1390 : Loss 2995.3821\n",
      "Iteration 1400 : Loss 2994.4296\n",
      "Iteration 1410 : Loss 2993.4866\n",
      "Iteration 1420 : Loss 2992.5530\n",
      "Iteration 1430 : Loss 2991.6288\n",
      "Iteration 1440 : Loss 2990.7137\n",
      "Iteration 1450 : Loss 2989.8078\n",
      "Iteration 1460 : Loss 2988.9109\n",
      "Iteration 1470 : Loss 2988.0229\n",
      "Iteration 1480 : Loss 2987.1437\n",
      "Iteration 1490 : Loss 2986.2733\n",
      "Iteration 1500 : Loss 2985.4115\n",
      "Iteration 1510 : Loss 2984.5582\n",
      "Iteration 1520 : Loss 2983.7134\n",
      "Iteration 1530 : Loss 2982.8769\n",
      "Iteration 1540 : Loss 2982.0488\n",
      "Iteration 1550 : Loss 2981.2288\n",
      "Iteration 1560 : Loss 2980.4169\n",
      "Iteration 1570 : Loss 2979.6130\n",
      "Iteration 1580 : Loss 2978.8170\n",
      "Iteration 1590 : Loss 2978.0289\n",
      "Iteration 1600 : Loss 2977.2485\n",
      "Iteration 1610 : Loss 2976.4758\n",
      "Iteration 1620 : Loss 2975.7107\n",
      "Iteration 1630 : Loss 2974.9532\n",
      "Iteration 1640 : Loss 2974.2030\n",
      "Iteration 1650 : Loss 2973.4602\n",
      "Iteration 1660 : Loss 2972.7247\n",
      "Iteration 1670 : Loss 2971.9964\n",
      "Iteration 1680 : Loss 2971.2753\n",
      "Iteration 1690 : Loss 2970.5612\n",
      "Iteration 1700 : Loss 2969.8540\n",
      "Iteration 1710 : Loss 2969.1538\n",
      "Iteration 1720 : Loss 2968.4604\n",
      "Iteration 1730 : Loss 2967.7738\n",
      "Iteration 1740 : Loss 2967.0939\n",
      "Iteration 1750 : Loss 2966.4206\n",
      "Iteration 1760 : Loss 2965.7538\n",
      "Iteration 1770 : Loss 2965.0936\n",
      "Iteration 1780 : Loss 2964.4398\n",
      "Iteration 1790 : Loss 2963.7923\n",
      "Iteration 1800 : Loss 2963.1511\n",
      "Iteration 1810 : Loss 2962.5162\n",
      "Iteration 1820 : Loss 2961.8874\n",
      "Iteration 1830 : Loss 2961.2647\n",
      "Iteration 1840 : Loss 2960.6480\n",
      "Iteration 1850 : Loss 2960.0373\n",
      "Iteration 1860 : Loss 2959.4326\n",
      "Iteration 1870 : Loss 2958.8336\n",
      "Iteration 1880 : Loss 2958.2405\n",
      "Iteration 1890 : Loss 2957.6531\n",
      "Iteration 1900 : Loss 2957.0714\n",
      "Iteration 1910 : Loss 2956.4953\n",
      "Iteration 1920 : Loss 2955.9247\n",
      "Iteration 1930 : Loss 2955.3597\n",
      "Iteration 1940 : Loss 2954.8001\n",
      "Iteration 1950 : Loss 2954.2459\n",
      "Iteration 1960 : Loss 2953.6970\n",
      "Iteration 1970 : Loss 2953.1534\n",
      "Iteration 1980 : Loss 2952.6151\n",
      "Iteration 1990 : Loss 2952.0819\n",
      "Iteration 2000 : Loss 2951.5538\n",
      "Iteration 2010 : Loss 2951.0308\n",
      "Iteration 2020 : Loss 2950.5128\n",
      "Iteration 2030 : Loss 2949.9998\n",
      "Iteration 2040 : Loss 2949.4918\n",
      "Iteration 2050 : Loss 2948.9885\n",
      "Iteration 2060 : Loss 2948.4901\n",
      "Iteration 2070 : Loss 2947.9965\n",
      "Iteration 2080 : Loss 2947.5076\n",
      "Iteration 2090 : Loss 2947.0234\n",
      "Iteration 2100 : Loss 2946.5438\n",
      "Iteration 2110 : Loss 2946.0688\n",
      "Iteration 2120 : Loss 2945.5983\n",
      "Iteration 2130 : Loss 2945.1323\n",
      "Iteration 2140 : Loss 2944.6708\n",
      "Iteration 2150 : Loss 2944.2136\n",
      "Iteration 2160 : Loss 2943.7608\n",
      "Iteration 2170 : Loss 2943.3124\n",
      "Iteration 2180 : Loss 2942.8682\n",
      "Iteration 2190 : Loss 2942.4282\n",
      "Iteration 2200 : Loss 2941.9924\n",
      "Iteration 2210 : Loss 2941.5608\n",
      "Iteration 2220 : Loss 2941.1332\n",
      "Iteration 2230 : Loss 2940.7097\n",
      "Iteration 2240 : Loss 2940.2903\n",
      "Iteration 2250 : Loss 2939.8748\n",
      "Iteration 2260 : Loss 2939.4633\n",
      "Iteration 2270 : Loss 2939.0556\n",
      "Iteration 2280 : Loss 2938.6519\n",
      "Iteration 2290 : Loss 2938.2519\n",
      "Iteration 2300 : Loss 2937.8557\n",
      "Iteration 2310 : Loss 2937.4633\n",
      "Iteration 2320 : Loss 2937.0746\n",
      "Iteration 2330 : Loss 2936.6896\n",
      "Iteration 2340 : Loss 2936.3082\n",
      "Iteration 2350 : Loss 2935.9304\n",
      "Iteration 2360 : Loss 2935.5562\n",
      "Iteration 2370 : Loss 2935.1855\n",
      "Iteration 2380 : Loss 2934.8183\n",
      "Iteration 2390 : Loss 2934.4546\n",
      "Iteration 2400 : Loss 2934.0943\n",
      "Iteration 2410 : Loss 2933.7374\n",
      "Iteration 2420 : Loss 2933.3838\n",
      "Iteration 2430 : Loss 2933.0336\n",
      "Iteration 2440 : Loss 2932.6867\n",
      "Iteration 2450 : Loss 2932.3430\n",
      "Iteration 2460 : Loss 2932.0026\n",
      "Iteration 2470 : Loss 2931.6654\n",
      "Iteration 2480 : Loss 2931.3313\n",
      "Iteration 2490 : Loss 2931.0004\n",
      "Iteration 2500 : Loss 2930.6725\n",
      "Iteration 2510 : Loss 2930.3478\n",
      "Iteration 2520 : Loss 2930.0261\n",
      "Iteration 2530 : Loss 2929.7074\n",
      "Iteration 2540 : Loss 2929.3916\n",
      "Iteration 2550 : Loss 2929.0789\n",
      "Iteration 2560 : Loss 2928.7691\n",
      "Iteration 2570 : Loss 2928.4621\n",
      "Iteration 2580 : Loss 2928.1581\n",
      "Iteration 2590 : Loss 2927.8568\n",
      "Iteration 2600 : Loss 2927.5584\n",
      "Iteration 2610 : Loss 2927.2628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2620 : Loss 2926.9699\n",
      "Iteration 2630 : Loss 2926.6798\n",
      "Iteration 2640 : Loss 2926.3924\n",
      "Iteration 2650 : Loss 2926.1076\n",
      "Iteration 2660 : Loss 2925.8255\n",
      "Iteration 2670 : Loss 2925.5460\n",
      "Iteration 2680 : Loss 2925.2692\n",
      "Iteration 2690 : Loss 2924.9949\n",
      "Iteration 2700 : Loss 2924.7231\n",
      "Iteration 2710 : Loss 2924.4539\n",
      "Iteration 2720 : Loss 2924.1872\n",
      "Iteration 2730 : Loss 2923.9229\n",
      "Iteration 2740 : Loss 2923.6611\n",
      "Iteration 2750 : Loss 2923.4018\n",
      "Iteration 2760 : Loss 2923.1448\n",
      "Iteration 2770 : Loss 2922.8902\n",
      "Iteration 2780 : Loss 2922.6380\n",
      "Iteration 2790 : Loss 2922.3881\n",
      "Iteration 2800 : Loss 2922.1406\n",
      "Iteration 2810 : Loss 2921.8953\n",
      "Iteration 2820 : Loss 2921.6523\n",
      "Iteration 2830 : Loss 2921.4115\n",
      "Iteration 2840 : Loss 2921.1730\n",
      "Iteration 2850 : Loss 2920.9366\n",
      "Iteration 2860 : Loss 2920.7025\n",
      "Iteration 2870 : Loss 2920.4705\n",
      "Iteration 2880 : Loss 2920.2406\n",
      "Iteration 2890 : Loss 2920.0129\n",
      "Iteration 2900 : Loss 2919.7873\n",
      "Iteration 2910 : Loss 2919.5637\n",
      "Iteration 2920 : Loss 2919.3422\n",
      "Iteration 2930 : Loss 2919.1228\n",
      "Iteration 2940 : Loss 2918.9053\n",
      "Iteration 2950 : Loss 2918.6899\n",
      "Iteration 2960 : Loss 2918.4764\n",
      "Iteration 2970 : Loss 2918.2649\n",
      "Iteration 2980 : Loss 2918.0554\n",
      "Iteration 2990 : Loss 2917.8477\n",
      "Iteration 3000 : Loss 2917.6420\n",
      "Iteration 3010 : Loss 2917.4382\n",
      "Iteration 3020 : Loss 2917.2362\n",
      "Iteration 3030 : Loss 2917.0361\n",
      "Iteration 3040 : Loss 2916.8378\n",
      "Iteration 3050 : Loss 2916.6413\n",
      "Iteration 3060 : Loss 2916.4466\n",
      "Iteration 3070 : Loss 2916.2538\n",
      "Iteration 3080 : Loss 2916.0626\n",
      "Iteration 3090 : Loss 2915.8732\n",
      "Iteration 3100 : Loss 2915.6856\n",
      "Iteration 3110 : Loss 2915.4996\n",
      "Iteration 3120 : Loss 2915.3154\n",
      "Iteration 3130 : Loss 2915.1328\n",
      "Iteration 3140 : Loss 2914.9519\n",
      "Iteration 3150 : Loss 2914.7727\n",
      "Iteration 3160 : Loss 2914.5951\n",
      "Iteration 3170 : Loss 2914.4191\n",
      "Iteration 3180 : Loss 2914.2447\n",
      "Iteration 3190 : Loss 2914.0719\n",
      "Iteration 3200 : Loss 2913.9007\n",
      "Iteration 3210 : Loss 2913.7310\n",
      "Iteration 3220 : Loss 2913.5628\n",
      "Iteration 3230 : Loss 2913.3962\n",
      "Iteration 3240 : Loss 2913.2311\n",
      "Iteration 3250 : Loss 2913.0675\n",
      "Iteration 3260 : Loss 2912.9054\n",
      "Iteration 3270 : Loss 2912.7448\n",
      "Iteration 3280 : Loss 2912.5856\n",
      "Iteration 3290 : Loss 2912.4278\n",
      "Iteration 3300 : Loss 2912.2715\n",
      "Iteration 3310 : Loss 2912.1166\n",
      "Iteration 3320 : Loss 2911.9631\n",
      "Iteration 3330 : Loss 2911.8110\n",
      "Iteration 3340 : Loss 2911.6603\n",
      "Iteration 3350 : Loss 2911.5109\n",
      "Iteration 3360 : Loss 2911.3629\n",
      "Iteration 3370 : Loss 2911.2162\n",
      "Iteration 3380 : Loss 2911.0708\n",
      "Iteration 3390 : Loss 2910.9267\n",
      "Iteration 3400 : Loss 2910.7840\n",
      "Iteration 3410 : Loss 2910.6425\n",
      "Iteration 3420 : Loss 2910.5023\n",
      "Iteration 3430 : Loss 2910.3634\n",
      "Iteration 3440 : Loss 2910.2257\n",
      "Iteration 3450 : Loss 2910.0893\n",
      "Iteration 3460 : Loss 2909.9540\n",
      "Iteration 3470 : Loss 2909.8200\n",
      "Iteration 3480 : Loss 2909.6872\n",
      "Iteration 3490 : Loss 2909.5556\n",
      "Iteration 3500 : Loss 2909.4252\n",
      "Iteration 3510 : Loss 2909.2959\n",
      "Iteration 3520 : Loss 2909.1679\n",
      "Iteration 3530 : Loss 2909.0409\n",
      "Iteration 3540 : Loss 2908.9151\n",
      "Iteration 3550 : Loss 2908.7904\n",
      "Iteration 3560 : Loss 2908.6668\n",
      "Iteration 3570 : Loss 2908.5444\n",
      "Iteration 3580 : Loss 2908.4230\n",
      "Iteration 3590 : Loss 2908.3027\n",
      "Iteration 3600 : Loss 2908.1835\n",
      "Iteration 3610 : Loss 2908.0654\n",
      "Iteration 3620 : Loss 2907.9483\n",
      "Iteration 3630 : Loss 2907.8323\n",
      "Iteration 3640 : Loss 2907.7173\n",
      "Iteration 3650 : Loss 2907.6033\n",
      "Iteration 3660 : Loss 2907.4903\n",
      "Iteration 3670 : Loss 2907.3783\n",
      "Iteration 3680 : Loss 2907.2674\n",
      "Iteration 3690 : Loss 2907.1574\n",
      "Iteration 3700 : Loss 2907.0484\n",
      "Iteration 3710 : Loss 2906.9404\n",
      "Iteration 3720 : Loss 2906.8333\n",
      "Iteration 3730 : Loss 2906.7272\n",
      "Iteration 3740 : Loss 2906.6220\n",
      "Iteration 3750 : Loss 2906.5178\n",
      "Iteration 3760 : Loss 2906.4145\n",
      "Iteration 3770 : Loss 2906.3121\n",
      "Iteration 3780 : Loss 2906.2106\n",
      "Iteration 3790 : Loss 2906.1100\n",
      "Iteration 3800 : Loss 2906.0103\n",
      "Iteration 3810 : Loss 2905.9115\n",
      "Iteration 3820 : Loss 2905.8135\n",
      "Iteration 3830 : Loss 2905.7165\n",
      "Iteration 3840 : Loss 2905.6202\n",
      "Iteration 3850 : Loss 2905.5249\n",
      "Iteration 3860 : Loss 2905.4304\n",
      "Iteration 3870 : Loss 2905.3367\n",
      "Iteration 3880 : Loss 2905.2438\n",
      "Iteration 3890 : Loss 2905.1518\n",
      "Iteration 3900 : Loss 2905.0605\n",
      "Iteration 3910 : Loss 2904.9701\n",
      "Iteration 3920 : Loss 2904.8805\n",
      "Iteration 3930 : Loss 2904.7916\n",
      "Iteration 3940 : Loss 2904.7036\n",
      "Iteration 3950 : Loss 2904.6163\n",
      "Iteration 3960 : Loss 2904.5298\n",
      "Iteration 3970 : Loss 2904.4440\n",
      "Iteration 3980 : Loss 2904.3590\n",
      "Iteration 3990 : Loss 2904.2747\n",
      "Iteration 4000 : Loss 2904.1912\n",
      "Iteration 4010 : Loss 2904.1084\n",
      "Iteration 4020 : Loss 2904.0264\n",
      "Iteration 4030 : Loss 2903.9450\n",
      "Iteration 4040 : Loss 2903.8644\n",
      "Iteration 4050 : Loss 2903.7845\n",
      "Iteration 4060 : Loss 2903.7053\n",
      "Iteration 4070 : Loss 2903.6267\n",
      "Iteration 4080 : Loss 2903.5489\n",
      "Iteration 4090 : Loss 2903.4717\n",
      "Iteration 4100 : Loss 2903.3953\n",
      "Iteration 4110 : Loss 2903.3194\n",
      "Iteration 4120 : Loss 2903.2443\n",
      "Iteration 4130 : Loss 2903.1698\n",
      "Iteration 4140 : Loss 2903.0959\n",
      "Iteration 4150 : Loss 2903.0227\n",
      "Iteration 4160 : Loss 2902.9502\n",
      "Iteration 4170 : Loss 2902.8782\n",
      "Iteration 4180 : Loss 2902.8069\n",
      "Iteration 4190 : Loss 2902.7362\n",
      "Iteration 4200 : Loss 2902.6662\n",
      "Iteration 4210 : Loss 2902.5967\n",
      "Iteration 4220 : Loss 2902.5278\n",
      "Iteration 4230 : Loss 2902.4596\n",
      "Iteration 4240 : Loss 2902.3919\n",
      "Iteration 4250 : Loss 2902.3248\n",
      "Iteration 4260 : Loss 2902.2583\n",
      "Iteration 4270 : Loss 2902.1924\n",
      "Iteration 4280 : Loss 2902.1271\n",
      "Iteration 4290 : Loss 2902.0623\n",
      "Iteration 4300 : Loss 2901.9980\n",
      "Iteration 4310 : Loss 2901.9344\n",
      "Iteration 4320 : Loss 2901.8713\n",
      "Iteration 4330 : Loss 2901.8087\n",
      "Iteration 4340 : Loss 2901.7467\n",
      "Iteration 4350 : Loss 2901.6852\n",
      "Iteration 4360 : Loss 2901.6242\n",
      "Iteration 4370 : Loss 2901.5638\n",
      "Iteration 4380 : Loss 2901.5039\n",
      "Iteration 4390 : Loss 2901.4445\n",
      "Iteration 4400 : Loss 2901.3856\n",
      "Iteration 4410 : Loss 2901.3272\n",
      "Iteration 4420 : Loss 2901.2693\n",
      "Iteration 4430 : Loss 2901.2119\n",
      "Iteration 4440 : Loss 2901.1551\n",
      "Iteration 4450 : Loss 2901.0987\n",
      "Iteration 4460 : Loss 2901.0428\n",
      "Iteration 4470 : Loss 2900.9873\n",
      "Iteration 4480 : Loss 2900.9324\n",
      "Iteration 4490 : Loss 2900.8779\n",
      "Iteration 4500 : Loss 2900.8239\n",
      "Iteration 4510 : Loss 2900.7703\n",
      "Iteration 4520 : Loss 2900.7172\n",
      "Iteration 4530 : Loss 2900.6646\n",
      "Iteration 4540 : Loss 2900.6124\n",
      "Iteration 4550 : Loss 2900.5607\n",
      "Iteration 4560 : Loss 2900.5094\n",
      "Iteration 4570 : Loss 2900.4585\n",
      "Iteration 4580 : Loss 2900.4081\n",
      "Iteration 4590 : Loss 2900.3581\n",
      "Iteration 4600 : Loss 2900.3086\n",
      "Iteration 4610 : Loss 2900.2594\n",
      "Iteration 4620 : Loss 2900.2107\n",
      "Iteration 4630 : Loss 2900.1624\n",
      "Iteration 4640 : Loss 2900.1145\n",
      "Iteration 4650 : Loss 2900.0670\n",
      "Iteration 4660 : Loss 2900.0200\n",
      "Iteration 4670 : Loss 2899.9733\n",
      "Iteration 4680 : Loss 2899.9270\n",
      "Iteration 4690 : Loss 2899.8811\n",
      "Iteration 4700 : Loss 2899.8356\n",
      "Iteration 4710 : Loss 2899.7905\n",
      "Iteration 4720 : Loss 2899.7458\n",
      "Iteration 4730 : Loss 2899.7014\n",
      "Iteration 4740 : Loss 2899.6575\n",
      "Iteration 4750 : Loss 2899.6139\n",
      "Iteration 4760 : Loss 2899.5707\n",
      "Iteration 4770 : Loss 2899.5278\n",
      "Iteration 4780 : Loss 2899.4853\n",
      "Iteration 4790 : Loss 2899.4432\n",
      "Iteration 4800 : Loss 2899.4014\n",
      "Iteration 4810 : Loss 2899.3600\n",
      "Iteration 4820 : Loss 2899.3189\n",
      "Iteration 4830 : Loss 2899.2782\n",
      "Iteration 4840 : Loss 2899.2378\n",
      "Iteration 4850 : Loss 2899.1977\n",
      "Iteration 4860 : Loss 2899.1580\n",
      "Iteration 4870 : Loss 2899.1187\n",
      "Iteration 4880 : Loss 2899.0796\n",
      "Iteration 4890 : Loss 2899.0409\n",
      "Iteration 4900 : Loss 2899.0025\n",
      "Iteration 4910 : Loss 2898.9645\n",
      "Iteration 4920 : Loss 2898.9267\n",
      "Iteration 4930 : Loss 2898.8893\n",
      "Iteration 4940 : Loss 2898.8522\n",
      "Iteration 4950 : Loss 2898.8154\n",
      "Iteration 4960 : Loss 2898.7789\n",
      "Iteration 4970 : Loss 2898.7427\n",
      "Iteration 4980 : Loss 2898.7069\n",
      "Iteration 4990 : Loss 2898.6713\n",
      "Iteration 5000 : Loss 2898.6360\n",
      "Iteration 5010 : Loss 2898.6010\n",
      "Iteration 5020 : Loss 2898.5664\n",
      "Iteration 5030 : Loss 2898.5320\n",
      "Iteration 5040 : Loss 2898.4979\n",
      "Iteration 5050 : Loss 2898.4640\n",
      "Iteration 5060 : Loss 2898.4305\n",
      "Iteration 5070 : Loss 2898.3972\n",
      "Iteration 5080 : Loss 2898.3642\n",
      "Iteration 5090 : Loss 2898.3315\n",
      "Iteration 5100 : Loss 2898.2991\n",
      "Iteration 5110 : Loss 2898.2669\n",
      "Iteration 5120 : Loss 2898.2351\n",
      "Iteration 5130 : Loss 2898.2034\n",
      "Iteration 5140 : Loss 2898.1721\n",
      "Iteration 5150 : Loss 2898.1410\n",
      "Iteration 5160 : Loss 2898.1101\n",
      "Iteration 5170 : Loss 2898.0795\n",
      "Iteration 5180 : Loss 2898.0492\n",
      "Iteration 5190 : Loss 2898.0191\n",
      "Iteration 5200 : Loss 2897.9893\n",
      "Iteration 5210 : Loss 2897.9597\n",
      "Iteration 5220 : Loss 2897.9304\n",
      "Iteration 5230 : Loss 2897.9013\n",
      "Iteration 5240 : Loss 2897.8724\n",
      "Iteration 5250 : Loss 2897.8438\n",
      "Iteration 5260 : Loss 2897.8154\n",
      "Iteration 5270 : Loss 2897.7873\n",
      "Iteration 5280 : Loss 2897.7594\n",
      "Iteration 5290 : Loss 2897.7317\n",
      "Iteration 5300 : Loss 2897.7043\n",
      "Iteration 5310 : Loss 2897.6770\n",
      "Iteration 5320 : Loss 2897.6500\n",
      "Iteration 5330 : Loss 2897.6233\n",
      "Iteration 5340 : Loss 2897.5967\n",
      "Iteration 5350 : Loss 2897.5704\n",
      "Iteration 5360 : Loss 2897.5443\n",
      "Iteration 5370 : Loss 2897.5184\n",
      "Iteration 5380 : Loss 2897.4927\n",
      "Iteration 5390 : Loss 2897.4672\n",
      "Iteration 5400 : Loss 2897.4419\n",
      "Iteration 5410 : Loss 2897.4169\n",
      "Iteration 5420 : Loss 2897.3920\n",
      "Iteration 5430 : Loss 2897.3674\n",
      "Iteration 5440 : Loss 2897.3429\n",
      "Iteration 5450 : Loss 2897.3187\n",
      "Iteration 5460 : Loss 2897.2946\n",
      "Iteration 5470 : Loss 2897.2708\n",
      "Iteration 5480 : Loss 2897.2471\n",
      "Iteration 5490 : Loss 2897.2237\n",
      "Iteration 5500 : Loss 2897.2004\n",
      "Iteration 5510 : Loss 2897.1773\n",
      "Iteration 5520 : Loss 2897.1544\n",
      "Iteration 5530 : Loss 2897.1317\n",
      "Iteration 5540 : Loss 2897.1092\n",
      "Iteration 5550 : Loss 2897.0869\n",
      "Iteration 5560 : Loss 2897.0647\n",
      "Iteration 5570 : Loss 2897.0428\n",
      "Iteration 5580 : Loss 2897.0210\n",
      "Iteration 5590 : Loss 2896.9994\n",
      "Iteration 5600 : Loss 2896.9779\n",
      "Iteration 5610 : Loss 2896.9567\n",
      "Iteration 5620 : Loss 2896.9356\n",
      "Iteration 5630 : Loss 2896.9147\n",
      "Iteration 5640 : Loss 2896.8939\n",
      "Iteration 5650 : Loss 2896.8733\n",
      "Iteration 5660 : Loss 2896.8529\n",
      "Iteration 5670 : Loss 2896.8327\n",
      "Iteration 5680 : Loss 2896.8126\n",
      "Iteration 5690 : Loss 2896.7927\n",
      "Iteration 5700 : Loss 2896.7729\n",
      "Iteration 5710 : Loss 2896.7533\n",
      "Iteration 5720 : Loss 2896.7339\n",
      "Iteration 5730 : Loss 2896.7146\n",
      "Iteration 5740 : Loss 2896.6955\n",
      "Iteration 5750 : Loss 2896.6765\n",
      "Iteration 5760 : Loss 2896.6577\n",
      "Iteration 5770 : Loss 2896.6390\n",
      "Iteration 5780 : Loss 2896.6205\n",
      "Iteration 5790 : Loss 2896.6021\n",
      "Iteration 5800 : Loss 2896.5839\n",
      "Iteration 5810 : Loss 2896.5658\n",
      "Iteration 5820 : Loss 2896.5479\n",
      "Iteration 5830 : Loss 2896.5301\n",
      "Iteration 5840 : Loss 2896.5125\n",
      "Iteration 5850 : Loss 2896.4950\n",
      "Iteration 5860 : Loss 2896.4776\n",
      "Iteration 5870 : Loss 2896.4604\n",
      "Iteration 5880 : Loss 2896.4434\n",
      "Iteration 5890 : Loss 2896.4264\n",
      "Iteration 5900 : Loss 2896.4096\n",
      "Iteration 5910 : Loss 2896.3929\n",
      "Iteration 5920 : Loss 2896.3764\n",
      "Iteration 5930 : Loss 2896.3600\n",
      "Iteration 5940 : Loss 2896.3437\n",
      "Iteration 5950 : Loss 2896.3276\n",
      "Iteration 5960 : Loss 2896.3116\n",
      "Iteration 5970 : Loss 2896.2957\n",
      "Iteration 5980 : Loss 2896.2799\n",
      "Iteration 5990 : Loss 2896.2643\n",
      "Iteration 6000 : Loss 2896.2488\n",
      "Iteration 6010 : Loss 2896.2334\n",
      "Iteration 6020 : Loss 2896.2181\n",
      "Iteration 6030 : Loss 2896.2030\n",
      "Iteration 6040 : Loss 2896.1880\n",
      "Iteration 6050 : Loss 2896.1731\n",
      "Iteration 6060 : Loss 2896.1583\n",
      "Iteration 6070 : Loss 2896.1436\n",
      "Iteration 6080 : Loss 2896.1291\n",
      "Iteration 6090 : Loss 2896.1146\n",
      "Iteration 6100 : Loss 2896.1003\n",
      "Iteration 6110 : Loss 2896.0861\n",
      "Iteration 6120 : Loss 2896.0720\n",
      "Iteration 6130 : Loss 2896.0580\n",
      "Iteration 6140 : Loss 2896.0441\n",
      "Iteration 6150 : Loss 2896.0304\n",
      "Iteration 6160 : Loss 2896.0167\n",
      "Iteration 6170 : Loss 2896.0032\n",
      "Iteration 6180 : Loss 2895.9897\n",
      "Iteration 6190 : Loss 2895.9764\n",
      "Iteration 6200 : Loss 2895.9632\n",
      "Iteration 6210 : Loss 2895.9501\n",
      "Iteration 6220 : Loss 2895.9370\n",
      "Iteration 6230 : Loss 2895.9241\n",
      "Iteration 6240 : Loss 2895.9113\n",
      "Iteration 6250 : Loss 2895.8986\n",
      "Iteration 6260 : Loss 2895.8860\n",
      "Iteration 6270 : Loss 2895.8734\n",
      "Iteration 6280 : Loss 2895.8610\n",
      "Iteration 6290 : Loss 2895.8487\n",
      "Iteration 6300 : Loss 2895.8365\n",
      "Iteration 6310 : Loss 2895.8244\n",
      "Iteration 6320 : Loss 2895.8123\n",
      "Iteration 6330 : Loss 2895.8004\n",
      "Iteration 6340 : Loss 2895.7885\n",
      "Iteration 6350 : Loss 2895.7768\n",
      "Iteration 6360 : Loss 2895.7651\n",
      "Iteration 6370 : Loss 2895.7535\n",
      "Iteration 6380 : Loss 2895.7420\n",
      "Iteration 6390 : Loss 2895.7307\n",
      "Iteration 6400 : Loss 2895.7194\n",
      "Iteration 6410 : Loss 2895.7081\n",
      "Iteration 6420 : Loss 2895.6970\n",
      "Iteration 6430 : Loss 2895.6860\n",
      "Iteration 6440 : Loss 2895.6750\n",
      "Iteration 6450 : Loss 2895.6641\n",
      "Iteration 6460 : Loss 2895.6533\n",
      "Iteration 6470 : Loss 2895.6426\n",
      "Iteration 6480 : Loss 2895.6320\n",
      "Iteration 6490 : Loss 2895.6215\n",
      "Iteration 6500 : Loss 2895.6110\n",
      "Iteration 6510 : Loss 2895.6006\n",
      "Iteration 6520 : Loss 2895.5903\n",
      "Iteration 6530 : Loss 2895.5801\n",
      "Iteration 6540 : Loss 2895.5700\n",
      "Iteration 6550 : Loss 2895.5599\n",
      "Iteration 6560 : Loss 2895.5499\n",
      "Iteration 6570 : Loss 2895.5400\n",
      "Iteration 6580 : Loss 2895.5302\n",
      "Iteration 6590 : Loss 2895.5204\n",
      "Iteration 6600 : Loss 2895.5107\n",
      "Iteration 6610 : Loss 2895.5011\n",
      "Iteration 6620 : Loss 2895.4916\n",
      "Iteration 6630 : Loss 2895.4821\n",
      "Iteration 6640 : Loss 2895.4727\n",
      "Iteration 6650 : Loss 2895.4634\n",
      "Iteration 6660 : Loss 2895.4542\n",
      "Iteration 6670 : Loss 2895.4450\n",
      "Iteration 6680 : Loss 2895.4359\n",
      "Iteration 6690 : Loss 2895.4269\n",
      "Iteration 6700 : Loss 2895.4179\n",
      "Iteration 6710 : Loss 2895.4090\n",
      "Iteration 6720 : Loss 2895.4002\n",
      "Iteration 6730 : Loss 2895.3914\n",
      "Iteration 6740 : Loss 2895.3827\n",
      "Iteration 6750 : Loss 2895.3741\n",
      "Iteration 6760 : Loss 2895.3655\n",
      "Iteration 6770 : Loss 2895.3570\n",
      "Iteration 6780 : Loss 2895.3486\n",
      "Iteration 6790 : Loss 2895.3402\n",
      "Iteration 6800 : Loss 2895.3319\n",
      "Iteration 6810 : Loss 2895.3236\n",
      "Iteration 6820 : Loss 2895.3154\n",
      "Iteration 6830 : Loss 2895.3073\n",
      "Iteration 6840 : Loss 2895.2992\n",
      "Iteration 6850 : Loss 2895.2912\n",
      "Iteration 6860 : Loss 2895.2833\n",
      "Iteration 6870 : Loss 2895.2754\n",
      "Iteration 6880 : Loss 2895.2676\n",
      "Iteration 6890 : Loss 2895.2598\n",
      "Iteration 6900 : Loss 2895.2521\n",
      "Iteration 6910 : Loss 2895.2444\n",
      "Iteration 6920 : Loss 2895.2369\n",
      "Iteration 6930 : Loss 2895.2293\n",
      "Iteration 6940 : Loss 2895.2218\n",
      "Iteration 6950 : Loss 2895.2144\n",
      "Iteration 6960 : Loss 2895.2070\n",
      "Iteration 6970 : Loss 2895.1997\n",
      "Iteration 6980 : Loss 2895.1925\n",
      "Iteration 6990 : Loss 2895.1853\n",
      "Iteration 7000 : Loss 2895.1781\n",
      "Iteration 7010 : Loss 2895.1710\n",
      "Iteration 7020 : Loss 2895.1639\n",
      "Iteration 7030 : Loss 2895.1569\n",
      "Iteration 7040 : Loss 2895.1500\n",
      "Iteration 7050 : Loss 2895.1431\n",
      "Iteration 7060 : Loss 2895.1363\n",
      "Iteration 7070 : Loss 2895.1295\n",
      "Iteration 7080 : Loss 2895.1227\n",
      "Iteration 7090 : Loss 2895.1160\n",
      "Iteration 7100 : Loss 2895.1094\n",
      "Iteration 7110 : Loss 2895.1028\n",
      "Iteration 7120 : Loss 2895.0962\n",
      "Iteration 7130 : Loss 2895.0897\n",
      "Iteration 7140 : Loss 2895.0833\n",
      "Iteration 7150 : Loss 2895.0769\n",
      "Iteration 7160 : Loss 2895.0705\n",
      "Iteration 7170 : Loss 2895.0642\n",
      "Iteration 7180 : Loss 2895.0580\n",
      "Iteration 7190 : Loss 2895.0517\n",
      "Iteration 7200 : Loss 2895.0456\n",
      "Iteration 7210 : Loss 2895.0394\n",
      "Iteration 7220 : Loss 2895.0333\n",
      "Iteration 7230 : Loss 2895.0273\n",
      "Iteration 7240 : Loss 2895.0213\n",
      "Iteration 7250 : Loss 2895.0153\n",
      "Iteration 7260 : Loss 2895.0094\n",
      "Iteration 7270 : Loss 2895.0036\n",
      "Iteration 7280 : Loss 2894.9977\n",
      "Iteration 7290 : Loss 2894.9919\n",
      "Iteration 7300 : Loss 2894.9862\n",
      "Iteration 7310 : Loss 2894.9805\n",
      "Iteration 7320 : Loss 2894.9748\n",
      "Iteration 7330 : Loss 2894.9692\n",
      "Iteration 7340 : Loss 2894.9636\n",
      "Iteration 7350 : Loss 2894.9581\n",
      "Iteration 7360 : Loss 2894.9526\n",
      "Iteration 7370 : Loss 2894.9471\n",
      "Iteration 7380 : Loss 2894.9417\n",
      "Iteration 7390 : Loss 2894.9363\n",
      "Iteration 7400 : Loss 2894.9309\n",
      "Iteration 7410 : Loss 2894.9256\n",
      "Iteration 7420 : Loss 2894.9204\n",
      "Iteration 7430 : Loss 2894.9151\n",
      "Iteration 7440 : Loss 2894.9099\n",
      "Iteration 7450 : Loss 2894.9048\n",
      "Iteration 7460 : Loss 2894.8996\n",
      "Iteration 7470 : Loss 2894.8945\n",
      "Iteration 7480 : Loss 2894.8895\n",
      "Iteration 7490 : Loss 2894.8845\n",
      "Iteration 7500 : Loss 2894.8795\n",
      "Iteration 7510 : Loss 2894.8745\n",
      "Iteration 7520 : Loss 2894.8696\n",
      "Iteration 7530 : Loss 2894.8647\n",
      "Iteration 7540 : Loss 2894.8599\n",
      "Iteration 7550 : Loss 2894.8551\n",
      "Iteration 7560 : Loss 2894.8503\n",
      "Iteration 7570 : Loss 2894.8456\n",
      "Iteration 7580 : Loss 2894.8408\n",
      "Iteration 7590 : Loss 2894.8362\n",
      "Iteration 7600 : Loss 2894.8315\n",
      "Iteration 7610 : Loss 2894.8269\n",
      "Iteration 7620 : Loss 2894.8223\n",
      "Iteration 7630 : Loss 2894.8177\n",
      "Iteration 7640 : Loss 2894.8132\n",
      "Iteration 7650 : Loss 2894.8087\n",
      "Iteration 7660 : Loss 2894.8043\n",
      "Iteration 7670 : Loss 2894.7998\n",
      "Iteration 7680 : Loss 2894.7954\n",
      "Iteration 7690 : Loss 2894.7911\n",
      "Iteration 7700 : Loss 2894.7867\n",
      "Iteration 7710 : Loss 2894.7824\n",
      "Iteration 7720 : Loss 2894.7781\n",
      "Iteration 7730 : Loss 2894.7739\n",
      "Iteration 7740 : Loss 2894.7696\n",
      "Iteration 7750 : Loss 2894.7654\n",
      "Iteration 7760 : Loss 2894.7613\n",
      "Iteration 7770 : Loss 2894.7571\n",
      "Iteration 7780 : Loss 2894.7530\n",
      "Iteration 7790 : Loss 2894.7489\n",
      "Iteration 7800 : Loss 2894.7449\n",
      "Iteration 7810 : Loss 2894.7408\n",
      "Iteration 7820 : Loss 2894.7368\n",
      "Iteration 7830 : Loss 2894.7329\n",
      "Iteration 7840 : Loss 2894.7289\n",
      "Iteration 7850 : Loss 2894.7250\n",
      "Iteration 7860 : Loss 2894.7211\n",
      "Iteration 7870 : Loss 2894.7172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7880 : Loss 2894.7134\n",
      "Iteration 7890 : Loss 2894.7095\n",
      "Iteration 7900 : Loss 2894.7057\n",
      "Iteration 7910 : Loss 2894.7020\n",
      "Iteration 7920 : Loss 2894.6982\n",
      "Iteration 7930 : Loss 2894.6945\n",
      "Iteration 7940 : Loss 2894.6908\n",
      "Iteration 7950 : Loss 2894.6871\n",
      "Iteration 7960 : Loss 2894.6835\n",
      "Iteration 7970 : Loss 2894.6798\n",
      "Iteration 7980 : Loss 2894.6762\n",
      "Iteration 7990 : Loss 2894.6726\n",
      "Iteration 8000 : Loss 2894.6691\n",
      "Iteration 8010 : Loss 2894.6655\n",
      "Iteration 8020 : Loss 2894.6620\n",
      "Iteration 8030 : Loss 2894.6585\n",
      "Iteration 8040 : Loss 2894.6551\n",
      "Iteration 8050 : Loss 2894.6516\n",
      "Iteration 8060 : Loss 2894.6482\n",
      "Iteration 8070 : Loss 2894.6448\n",
      "Iteration 8080 : Loss 2894.6414\n",
      "Iteration 8090 : Loss 2894.6380\n",
      "Iteration 8100 : Loss 2894.6347\n",
      "Iteration 8110 : Loss 2894.6314\n",
      "Iteration 8120 : Loss 2894.6281\n",
      "Iteration 8130 : Loss 2894.6248\n",
      "Iteration 8140 : Loss 2894.6215\n",
      "Iteration 8150 : Loss 2894.6183\n",
      "Iteration 8160 : Loss 2894.6151\n",
      "Iteration 8170 : Loss 2894.6119\n",
      "Iteration 8180 : Loss 2894.6087\n",
      "Iteration 8190 : Loss 2894.6055\n",
      "Iteration 8200 : Loss 2894.6024\n",
      "Iteration 8210 : Loss 2894.5993\n",
      "Iteration 8220 : Loss 2894.5962\n",
      "Iteration 8230 : Loss 2894.5931\n",
      "Iteration 8240 : Loss 2894.5900\n",
      "Iteration 8250 : Loss 2894.5870\n",
      "Iteration 8260 : Loss 2894.5840\n",
      "Iteration 8270 : Loss 2894.5810\n",
      "Iteration 8280 : Loss 2894.5780\n",
      "Iteration 8290 : Loss 2894.5750\n",
      "Iteration 8300 : Loss 2894.5720\n",
      "Iteration 8310 : Loss 2894.5691\n",
      "Iteration 8320 : Loss 2894.5662\n",
      "Iteration 8330 : Loss 2894.5633\n",
      "Iteration 8340 : Loss 2894.5604\n",
      "Iteration 8350 : Loss 2894.5575\n",
      "Iteration 8360 : Loss 2894.5547\n",
      "Iteration 8370 : Loss 2894.5519\n",
      "Iteration 8380 : Loss 2894.5490\n",
      "Iteration 8390 : Loss 2894.5462\n",
      "Iteration 8400 : Loss 2894.5434\n",
      "Iteration 8410 : Loss 2894.5407\n",
      "Iteration 8420 : Loss 2894.5379\n",
      "Iteration 8430 : Loss 2894.5352\n",
      "Iteration 8440 : Loss 2894.5325\n",
      "Iteration 8450 : Loss 2894.5298\n",
      "Iteration 8460 : Loss 2894.5271\n",
      "Iteration 8470 : Loss 2894.5244\n",
      "Iteration 8480 : Loss 2894.5217\n",
      "Iteration 8490 : Loss 2894.5191\n",
      "Iteration 8500 : Loss 2894.5165\n",
      "Iteration 8510 : Loss 2894.5139\n",
      "Iteration 8520 : Loss 2894.5113\n",
      "Iteration 8530 : Loss 2894.5087\n",
      "Iteration 8540 : Loss 2894.5061\n",
      "Iteration 8550 : Loss 2894.5035\n",
      "Iteration 8560 : Loss 2894.5010\n",
      "Iteration 8570 : Loss 2894.4985\n",
      "Iteration 8580 : Loss 2894.4960\n",
      "Iteration 8590 : Loss 2894.4935\n",
      "Iteration 8600 : Loss 2894.4910\n",
      "Iteration 8610 : Loss 2894.4885\n",
      "Iteration 8620 : Loss 2894.4860\n",
      "Iteration 8630 : Loss 2894.4836\n",
      "Iteration 8640 : Loss 2894.4812\n",
      "Iteration 8650 : Loss 2894.4787\n",
      "Iteration 8660 : Loss 2894.4763\n",
      "Iteration 8670 : Loss 2894.4739\n",
      "Iteration 8680 : Loss 2894.4716\n",
      "Iteration 8690 : Loss 2894.4692\n",
      "Iteration 8700 : Loss 2894.4668\n",
      "Iteration 8710 : Loss 2894.4645\n",
      "Iteration 8720 : Loss 2894.4622\n",
      "Iteration 8730 : Loss 2894.4598\n",
      "Iteration 8740 : Loss 2894.4575\n",
      "Iteration 8750 : Loss 2894.4552\n",
      "Iteration 8760 : Loss 2894.4529\n",
      "Iteration 8770 : Loss 2894.4507\n",
      "Iteration 8780 : Loss 2894.4484\n",
      "Iteration 8790 : Loss 2894.4462\n",
      "Iteration 8800 : Loss 2894.4439\n",
      "Iteration 8810 : Loss 2894.4417\n",
      "Iteration 8820 : Loss 2894.4395\n",
      "Iteration 8830 : Loss 2894.4373\n",
      "Iteration 8840 : Loss 2894.4351\n",
      "Iteration 8850 : Loss 2894.4329\n",
      "Iteration 8860 : Loss 2894.4307\n",
      "Iteration 8870 : Loss 2894.4286\n",
      "Iteration 8880 : Loss 2894.4264\n",
      "Iteration 8890 : Loss 2894.4243\n",
      "Iteration 8900 : Loss 2894.4222\n",
      "Iteration 8910 : Loss 2894.4201\n",
      "Iteration 8920 : Loss 2894.4180\n",
      "Iteration 8930 : Loss 2894.4159\n",
      "Iteration 8940 : Loss 2894.4138\n",
      "Iteration 8950 : Loss 2894.4117\n",
      "Iteration 8960 : Loss 2894.4096\n",
      "Iteration 8970 : Loss 2894.4076\n",
      "Iteration 8980 : Loss 2894.4055\n",
      "Iteration 8990 : Loss 2894.4035\n",
      "Iteration 9000 : Loss 2894.4015\n",
      "Iteration 9010 : Loss 2894.3994\n",
      "Iteration 9020 : Loss 2894.3974\n",
      "Iteration 9030 : Loss 2894.3954\n",
      "Iteration 9040 : Loss 2894.3934\n",
      "Iteration 9050 : Loss 2894.3915\n",
      "Iteration 9060 : Loss 2894.3895\n",
      "Iteration 9070 : Loss 2894.3875\n",
      "Iteration 9080 : Loss 2894.3856\n",
      "Iteration 9090 : Loss 2894.3836\n",
      "Iteration 9100 : Loss 2894.3817\n",
      "Iteration 9110 : Loss 2894.3798\n",
      "Iteration 9120 : Loss 2894.3778\n",
      "Iteration 9130 : Loss 2894.3759\n",
      "Iteration 9140 : Loss 2894.3740\n",
      "Iteration 9150 : Loss 2894.3721\n",
      "Iteration 9160 : Loss 2894.3703\n",
      "Iteration 9170 : Loss 2894.3684\n",
      "Iteration 9180 : Loss 2894.3665\n",
      "Iteration 9190 : Loss 2894.3646\n",
      "Iteration 9200 : Loss 2894.3628\n",
      "Iteration 9210 : Loss 2894.3609\n",
      "Iteration 9220 : Loss 2894.3591\n",
      "Iteration 9230 : Loss 2894.3573\n",
      "Iteration 9240 : Loss 2894.3555\n",
      "Iteration 9250 : Loss 2894.3536\n",
      "Iteration 9260 : Loss 2894.3518\n",
      "Iteration 9270 : Loss 2894.3500\n",
      "Iteration 9280 : Loss 2894.3482\n",
      "Iteration 9290 : Loss 2894.3465\n",
      "Iteration 9300 : Loss 2894.3447\n",
      "Iteration 9310 : Loss 2894.3429\n",
      "Iteration 9320 : Loss 2894.3411\n",
      "Iteration 9330 : Loss 2894.3394\n",
      "Iteration 9340 : Loss 2894.3376\n",
      "Iteration 9350 : Loss 2894.3359\n",
      "Iteration 9360 : Loss 2894.3342\n",
      "Iteration 9370 : Loss 2894.3324\n",
      "Iteration 9380 : Loss 2894.3307\n",
      "Iteration 9390 : Loss 2894.3290\n",
      "Iteration 9400 : Loss 2894.3273\n",
      "Iteration 9410 : Loss 2894.3256\n",
      "Iteration 9420 : Loss 2894.3239\n",
      "Iteration 9430 : Loss 2894.3222\n",
      "Iteration 9440 : Loss 2894.3205\n",
      "Iteration 9450 : Loss 2894.3188\n",
      "Iteration 9460 : Loss 2894.3172\n",
      "Iteration 9470 : Loss 2894.3155\n",
      "Iteration 9480 : Loss 2894.3139\n",
      "Iteration 9490 : Loss 2894.3122\n",
      "Iteration 9500 : Loss 2894.3106\n",
      "Iteration 9510 : Loss 2894.3089\n",
      "Iteration 9520 : Loss 2894.3073\n",
      "Iteration 9530 : Loss 2894.3056\n",
      "Iteration 9540 : Loss 2894.3040\n",
      "Iteration 9550 : Loss 2894.3024\n",
      "Iteration 9560 : Loss 2894.3008\n",
      "Iteration 9570 : Loss 2894.2992\n",
      "Iteration 9580 : Loss 2894.2976\n",
      "Iteration 9590 : Loss 2894.2960\n",
      "Iteration 9600 : Loss 2894.2944\n",
      "Iteration 9610 : Loss 2894.2928\n",
      "Iteration 9620 : Loss 2894.2912\n",
      "Iteration 9630 : Loss 2894.2897\n",
      "Iteration 9640 : Loss 2894.2881\n",
      "Iteration 9650 : Loss 2894.2865\n",
      "Iteration 9660 : Loss 2894.2850\n",
      "Iteration 9670 : Loss 2894.2834\n",
      "Iteration 9680 : Loss 2894.2819\n",
      "Iteration 9690 : Loss 2894.2803\n",
      "Iteration 9700 : Loss 2894.2788\n",
      "Iteration 9710 : Loss 2894.2773\n",
      "Iteration 9720 : Loss 2894.2757\n",
      "Iteration 9730 : Loss 2894.2742\n",
      "Iteration 9740 : Loss 2894.2727\n",
      "Iteration 9750 : Loss 2894.2712\n",
      "Iteration 9760 : Loss 2894.2697\n",
      "Iteration 9770 : Loss 2894.2682\n",
      "Iteration 9780 : Loss 2894.2667\n",
      "Iteration 9790 : Loss 2894.2652\n",
      "Iteration 9800 : Loss 2894.2637\n",
      "Iteration 9810 : Loss 2894.2622\n",
      "Iteration 9820 : Loss 2894.2607\n",
      "Iteration 9830 : Loss 2894.2592\n",
      "Iteration 9840 : Loss 2894.2577\n",
      "Iteration 9850 : Loss 2894.2563\n",
      "Iteration 9860 : Loss 2894.2548\n",
      "Iteration 9870 : Loss 2894.2533\n",
      "Iteration 9880 : Loss 2894.2519\n",
      "Iteration 9890 : Loss 2894.2504\n",
      "Iteration 9900 : Loss 2894.2490\n",
      "Iteration 9910 : Loss 2894.2475\n",
      "Iteration 9920 : Loss 2894.2461\n",
      "Iteration 9930 : Loss 2894.2447\n",
      "Iteration 9940 : Loss 2894.2432\n",
      "Iteration 9950 : Loss 2894.2418\n",
      "Iteration 9960 : Loss 2894.2404\n",
      "Iteration 9970 : Loss 2894.2389\n",
      "Iteration 9980 : Loss 2894.2375\n",
      "Iteration 9990 : Loss 2894.2361\n",
      "Iteration 10000 : Loss 2894.2347\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X, W, b, y)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) test 데이터에 대한 성능 확인하기\n",
    "- test 데이터에 대한 성능을 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881.5383753343085"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "- x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], prediction, label=\"pred\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 및 정리\n",
    "- Learning Rate: 0.1 \n",
    "- loss: MSE(Mean Squared Error) \n",
    "- Iteration: 10,000\n",
    "- **Iteration 1350부터 Loss값 2999.2896 진입**\n",
    "- **Test Data에 대한 성능: 2881.5383**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
